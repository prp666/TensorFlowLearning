{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598c1dd8-6c74-41ad-8167-a177cf61c70b",
   "metadata": {},
   "source": [
    "## In Tensorflow.keras_learn(1) we have leraned how to compile, fit, how to build a model by keras.Moudle class, keras.Sequential, Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c738a-5846-4fad-b85d-b3d21a027ee7",
   "metadata": {},
   "source": [
    "### Now it's time to learn more, First we will learn how to modify hyperparameters in the model bu using `Keras Tuner`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cbb7bf-5ed3-429a-bd5b-2e461b62e750",
   "metadata": {},
   "source": [
    "#### First let's know what is hyperparameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909cf9b7-5de2-46ad-a934-8d838c3119ec",
   "metadata": {},
   "source": [
    "The hypterparameters are the parameters which won't changed while training, they control both training progress and parameters of model topo, they has two kinds:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098c0f58-7202-4839-a313-7e372a887f9a",
   "metadata": {},
   "source": [
    "1. Model Hyperparameters: This kind of hyperparameter will infuence the model complexity, like the width of hidden layer and the number of it, has the model using `Dropout`, `BatchNorm`? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15cfa17-a592-4bc7-a3bf-275bda676628",
   "metadata": {},
   "source": [
    "2. Algorithm Hyperparameters: Have influence in algotithm's speed and quality, like the `learning rate` and the k of `knn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c440bf38-7189-4420-82da-acf1d1c740c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1829362d-075b-41d3-a9d5-147932c07804",
   "metadata": {},
   "source": [
    " **The Tuner need be installed by pip**\n",
    "\n",
    " To make sure we only installed this in visual enviroment, we need to install it by command line, beacuase I am using the Raspberry pi for running this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e8596-57d9-4582-afef-c415f9007f95",
   "metadata": {},
   "source": [
    "#### The command in python is `pip install -U keras-tuner`, the `-U` is upgrade if there's latest one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42785281-71eb-4413-be5a-a7f85b28e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b425dd-5e74-45fb-b53c-1c9d0bb3cc08",
   "metadata": {},
   "source": [
    "We download Fashion MNIST dataset for this mission, the dataloading will be covered later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffe55419-2207-4afb-a4c4-74460705281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(img_train, lable_train), (img_test, lable_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea02f868-2c18-4570-be94-d4dc23733bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(len(img_train))\n",
    "print(len(lable_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6ebe62f-ac36-4044-8ce3-dc217d284276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(img_test))\n",
    "print(len(lable_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8843376c-afaf-472a-b8db-91c9cde1f33d",
   "metadata": {},
   "source": [
    "Now We need normalize the data to `0-1`, but by the way we need to know the dtype, because lots of the model only accept float32 or float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a56312b3-d030-4c35-ae5b-013c619e1eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_train[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce1c6a9c-3d5a-49b9-a87d-185a9493ba8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_test[0].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f456a766-fdf4-4af6-b1bb-a32e1513315f",
   "metadata": {},
   "source": [
    "The datatype is unsigned int 8-bit, which usually used in images, because this do not has symble like plus or minus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e25d341-200d-4acc-87a8-567d97341b87",
   "metadata": {},
   "source": [
    "**Because data which load by `keras.datasets`, it's always numpy array, so we can use `astype()` method for changing dtype, this can only used by numpy array, if the data is `tensorflow.tensors`, we can use `cast()` to change the dtype**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d943d250-fc42-43a5-b8b9-fde1b4994e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68ec6d66-ad51-4fee-9bc2-2292b16a947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = img_train.astype(np.float32) / 255.0\n",
    "img_test = img_test.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd81df15-6c67-443b-b404-0d899fe8de84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_train[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29085684-ae08-46a5-883a-8e386139b3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_test[0].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf6ff2-2fb8-4eae-8101-79446c04941a",
   "metadata": {},
   "source": [
    "### Now we need to define hyperparameter model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65377bba-bbb0-47cb-8811-7b365829425c",
   "metadata": {},
   "source": [
    "We not only need to define the model but also need to define the hyperparameter's searching space, we call the model and the configuration together \"hyperparameter searching model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3549b5f0-7240-43af-8cec-8942f4bb767f",
   "metadata": {},
   "source": [
    "There are two kinds of method to define a hyperparameter searching model:\n",
    "\n",
    "1. Using `model_builder(hp)` function for creating, `hp` is `keras_tuner.Hyperparameters()`'s object, which will automatically input by keras_tuner when calling `model_builder()`\n",
    "\n",
    "\n",
    "2. Using `keras Tuner API`, inside there's one `HyperModel` class, we can subclassing(extend, override) this class to create hyperparameter model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4412e4f0-4609-44c8-a808-73279a05a8ec",
   "metadata": {},
   "source": [
    "For **computer vision**, can using `HyperXception`, `HyperResNet` for finding best hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7895f97-292d-4e53-b3c3-cf913b31e3fe",
   "metadata": {},
   "source": [
    "#### The First method: Create a hyperparameter model by `model_builder(hp)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ee1ec8-ee47-47a5-a995-e147d7011ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Input(shape=(28, 28)))\n",
    "    model.add(keras.layers.Flatten())\n",
    "\n",
    "    # Now define the hyperparameters range of hidden layer, the output number of the hidden layer\n",
    "    hp_units = hp.Int(\"units\", min_value=32, max_value=512, step=32)\n",
    "    model.add(keras.layers.Dense(units=hp_units, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(10))\n",
    "\n",
    "    # Now define the hyperparameters range of learning rate for optimizer\n",
    "    hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "        # from_logits = True means input is logits, we haven't use softmax in the last layer, keras will helps us calculating possibilities\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3323e643-25db-4ee9-b18d-9f72b750d703",
   "metadata": {},
   "source": [
    "#### In the cell upward is method one, create a method called model_builder, there are something need to be noticed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c357e35-7029-4d9c-a76c-1cb5cc77fc44",
   "metadata": {},
   "source": [
    "1. For everymodel build by `tensorflow.keras`, we shoudn't use the `input_size` parameter now, the tensorflow official suggests using `keras.layers.Input()` to specify the inputsize.\n",
    "\n",
    "2. The hp, is an object which will be automatically created by `keras_tuner.Hyperparameters()`, this object covers useful classes we need in hyperparameter selecting.\n",
    "\n",
    "3. The hyperparameters in hiddenlayers and in algorithms can be changed by keras_tuner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcc565a-7c47-4d6c-ba69-d58c2c79966a",
   "metadata": {},
   "source": [
    "**The useful classes of object `hp`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef5077f-6f84-41f4-894b-0ce7041eb408",
   "metadata": {},
   "source": [
    "1. `hp.Int(name, min_value, max_value, step)`: This class will create a list of int value range from min_value to max_value, and we can also specify step, `eg:hp.Int(\"dense1\", 1, 10, 2) gives hyperparameters : [1, 3, 5, 7, 9]`\n",
    "\n",
    "2. `hp.Float(name, min_value, max_value, step, sampling)`: This class will create a list of float value range from min_value to max_value, also can specify the step, `parameter sampling` means how to select a hyperparameter, there are usually two values, `\"linear\"` means select value uniformed, good for **normalize, dropout**, `\"log\"` will first put value in log space and select, which perform well on **learning_rate**\n",
    "\n",
    "3. `hp.Choice(name, values)`: This class provides several values but the model or the algorithm only select one of them in one searching epoch, `eg:hp.choice(\"activation_fn\", [\"relu\", \"sigmoid\", \"softmax\", \"tanh\"])`\n",
    "\n",
    "\n",
    "There are more functions inside, but from now these are enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35636add-8a49-4680-8823-ad2fa13fb1a6",
   "metadata": {},
   "source": [
    "#### The second kind of method is subclassing the class `HyperModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad88723-0148-442f-8ddb-9b992b7017d1",
   "metadata": {},
   "source": [
    "The HyperModel is a abstract class of keras_tuner, we need to override the function `build(self, hp)` and define both the model and the hyperparameters' searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c54950c-7651-4d60-848e-077d10b530e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "938277b2-29ca-4676-bca5-1ca7ba163b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(kt.HyperModel):\n",
    "    \n",
    "    def build(self, hp):\n",
    "        inputs = keras.layers.Input(shape=(28, 28))\n",
    "        x = keras.layers.Flatten()(inputs)\n",
    "\n",
    "        # Define model Hyperparameters \n",
    "        units = hp.Int(\"units\", min_value=64, max_value=512, step=64)\n",
    "        dropout = hp.Float(\"dropout\", min_value=0.0, max_value=0.6, step=0.1)\n",
    "\n",
    "        x = keras.layers.Dense(units=units, activation=\"relu\")(x)\n",
    "        x = keras.layers.Dropout(dropout)(x)\n",
    "        outputs = keras.layers.Dense(10)(x)\n",
    "\n",
    "        model = keras.Model(inputs, outputs)\n",
    "\n",
    "        # Now the model has beem built, we need define algorithms' hyperparameters\n",
    "        optimizer_name = hp.Choice(\"optimizer\", [\"adam\", \"sgd\"])\n",
    "        if optimizer_name==\"adam\":\n",
    "            learning_rate = hp.Choice(\"learning_rate_adam\", [1e-3, 5e-4, 1e-4])\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "        else:\n",
    "            learning_rate = hp.Choice(\"learning_rate_sgd\", [1e-2, 5e-3, 1e-3])\n",
    "            momentum = hp.Choice(\"momentum\", [0.0, 0.9])\n",
    "            optimizer = keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    "        )\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863fcab-ed29-459a-97c7-7829187327e5",
   "metadata": {},
   "source": [
    "So the second method is similar with the first one, but the first one is very convinent, the second one can also do hyperparameter change with `model.fit()`, find the batch_size, callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83195520-d81a-4602-b673-b115f7592b4b",
   "metadata": {},
   "source": [
    "**We can find there are few same features of each method, both will create model, define hyperparameters(model and algorithm), and compile the model, prepare for the training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a94b00-4dfc-4d74-a2e5-935cfb9adf3f",
   "metadata": {},
   "source": [
    "### After define the model and define the hyperparameters, we need to instantiation the Tuner, `keras_tuners` offers four kinds of optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d669f52-6f04-41ff-9cbf-25e2279fb4e9",
   "metadata": {},
   "source": [
    "1. RandomSearch: The name is the method we using, by the randomly selecting the hyperparameters, good on small seaching area.\n",
    "\n",
    "2. Hyperband: More efficient than the RandomSearch.\n",
    "\n",
    "3. BayesianOptimization: Good performance on continues datasets.\n",
    "\n",
    "4. Sklearn: Can use the keras_tuner on the scikt_learn model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893098d-cbfa-42c7-b84d-b7c63a458bd2",
   "metadata": {},
   "source": [
    "Now we use the `Hyperband` in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bdad34-e079-41c3-ad13-0183c8d6ac87",
   "metadata": {},
   "source": [
    "**To instantiation the Hyperband tuner, must specify the `hyper model`, the `objective(metrics)` we want to optimize, and the `maximum epoches` of the tuner**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b91b275-ebf7-491f-997e-56fc7f847960",
   "metadata": {},
   "source": [
    "The parameters mean:\n",
    "\n",
    "1. hypermodel: Receive the model we want to optimize, includes `model_builder()` function and `HyperModel()` class objects.\n",
    "\n",
    "2. objective: The metrics we want to optimize, \"val_accuracy\" is for accuracy of the validation dataset.\n",
    "\n",
    "3. max_epochs: The number of epochs if objective haven't improve\n",
    "\n",
    "4. factor: Specify how much percentage we want to store, factor=3, so we divided data into 3 parts, only store 1 part of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfc73a15-9630-42bf-aa3b-6b3312007283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tuner by the model_builder(hp)\n",
    "tuner_1 = kt.Hyperband(\n",
    "    hypermodel=model_builder,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=10,\n",
    "    factor=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a437d322-7ea6-4f57-a886-2ac90f16770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tuner by the kt.HyperModel():\n",
    "tuner_2 = kt.Hyperband(\n",
    "    hypermodel=MyHyperModel(),\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=15,\n",
    "    factor=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fdeb4c-92eb-460f-9866-52684ca8bee2",
   "metadata": {},
   "source": [
    "#### In the `keras_tuner` and `tensorflow.keras.Module`, the `objective` and the `metrics` all can use `strings`, the metrics/objective using decided by the losses specified in the `compile()`, like if we use `accuracy`, keras will dive in `compile()` and find out which loss function we use, then automatically select the metrics, if use `val_accuracy`, also will automatically find it, but this only works on the `validation dataset` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6c5dcc-25e7-4b18-a8cc-49bc14a4070d",
   "metadata": {},
   "source": [
    "Now we don't want the procedure continues such long, so we add `Early stopping`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c003ec0d-b45e-49c8-b4d3-a4da4813dc57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
